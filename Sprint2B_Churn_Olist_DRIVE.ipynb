{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmsroysillerico/MAE_M13_PUB/blob/main/Sprint2B_Churn_Olist_DRIVE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a307dd9a",
      "metadata": {
        "id": "a307dd9a"
      },
      "source": [
        "\n",
        "# Sprint2B – Churn de Clientes Olist (Sprint 1 + Sprint 2)\n",
        "\n",
        "**Notebook:** `Sprint2B_Churn_Olist_DRIVE.ipynb`  \n",
        "**Proyecto:** Análisis de *churn* de clientes (clientes que dejan de comprar) – Dataset Olist  \n",
        "**Sprints cubiertos:**  \n",
        "- **Sprint 1:** EDA inicial, hipótesis, definición preliminar de target y métricas base.  \n",
        "- **Sprint 2:** Pipeline reproducible, feature engineering avanzado, definición de target final, modelos base y métricas de negocio.\n",
        "\n",
        "> Este notebook está pensado como una versión **end‑to‑end** del proyecto.  \n",
        "> El notebook original del Sprint 1 se mantiene separado como evidencia histórica.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "858c3a19",
      "metadata": {
        "id": "858c3a19"
      },
      "source": [
        "\n",
        "## 0. Agenda del Notebook\n",
        "\n",
        "1. **Contexto de negocio y objetivo del MVP**  \n",
        "2. **Configuración del entorno y rutas**  \n",
        "3. **Carga e inspección de datos Olist (9 CSV)**  \n",
        "4. **EDA inicial resumido (Sprint 1)**  \n",
        "   - RFM básico  \n",
        "   - Primeras métricas de churn  \n",
        "5. **Diseño del pipeline reproducible (Sprint 2)**  \n",
        "   - Funciones modulares: ingesta, limpieza, master table, features, target  \n",
        "6. **Construcción de Master Table a nivel pedido (`orders_enriched`)**  \n",
        "7. **Agregación por cliente y Feature Engineering ampliado (`customer_features`)**  \n",
        "   - Variables RFM  \n",
        "   - Variables logísticas, de reviews, pagos, diversidad de compras, etc.  \n",
        "8. **Definición de target de churn**  \n",
        "   - Target preliminar (Sprint 1)  \n",
        "   - Target final (Sprint 2)  \n",
        "9. **Selección de variables (modelo para “redimensionar” – Random Forest)**  \n",
        "10. **Modelos de clasificación (al menos dos modelos)**  \n",
        "    - Regresión logística  \n",
        "    - Random Forest  \n",
        "11. **Métricas técnicas y de negocio**  \n",
        "    - Accuracy, Precision, Recall, F1, ROC‑AUC  \n",
        "    - Churn rate, retención, análisis por segmentos  \n",
        "    - Ejemplos de correlación, chi‑cuadrado, ANOVA/F‑test  \n",
        "12. **Notas sobre versionamiento (Git) y pipeline mensual**  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22018cf3",
      "metadata": {
        "id": "22018cf3"
      },
      "source": [
        "\n",
        "## 1. Contexto de negocio y objetivo del MVP\n",
        "\n",
        "- **Problema de negocio:** Olist desea **identificar clientes con riesgo de abandono (churn)** para poder priorizar campañas de retención y evitar la pérdida de valor futuro.\n",
        "- **Unidad de análisis:** cliente final, identificado por `customer_unique_id`.\n",
        "- **Churn (concepto):** cliente que **compró en el pasado pero deja de comprar** por un período suficientemente largo como para considerarlo *cliente perdido* desde la perspectiva del negocio.\n",
        "- **Periodicidad objetivo:** análisis **mensual** (clientes activos vs inactivos dentro de ventanas temporales).\n",
        "\n",
        "### Objetivo del MVP\n",
        "\n",
        "El **MVP** del proyecto es un pipeline que, **cada mes**, sea capaz de:\n",
        "\n",
        "1. Leer los datos transaccionales de Olist (nuevos pedidos).  \n",
        "2. Construir automáticamente una **tabla maestra a nivel cliente** con variables de comportamiento (RFM, logística, satisfacción, etc.).  \n",
        "3. Generar una **probabilidad de churn** por cliente.  \n",
        "4. Entregar una **lista priorizada** de clientes en alto riesgo de abandono, junto con **KPIs de negocio** (churn rate, retención, valor monetario esperado, etc.).\n",
        "\n",
        "Este notebook implementa la versión **analítica y de prototipo** de ese MVP, cubriendo los entregables de Sprint 1 y Sprint 2.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2a68662",
      "metadata": {
        "id": "d2a68662"
      },
      "source": [
        "## 2. Configuración del entorno e imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4099842b",
      "metadata": {
        "id": "4099842b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Imports principales\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    roc_curve\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import chi2, f_classif\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.style.use(\"default\")\n",
        "pd.options.display.float_format = \"{:,.4f}\".format\n",
        "\n",
        "print(\"Entorno listo.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e18626be",
      "metadata": {
        "id": "e18626be"
      },
      "source": [
        "\n",
        "### 2.1. Configuración de rutas\n",
        "\n",
        "Ajustar la ruta base `DATA_DIR` según el entorno:\n",
        "\n",
        "- En **Google Drive / Colab**, por ejemplo:  \n",
        "  `DATA_DIR = Path('/content/drive/MyDrive/Maestria/M13/Olist')`\n",
        "- En entorno local, apuntar a la carpeta donde están los 9 CSV de Olist.\n",
        "\n",
        "En este notebook los archivos esperados son:\n",
        "\n",
        "- `olist_orders_dataset.csv`  \n",
        "- `olist_customers_dataset.csv`  \n",
        "- `olist_order_items_dataset.csv`  \n",
        "- `olist_order_payments_dataset.csv`  \n",
        "- `olist_order_reviews_dataset.csv`  \n",
        "- `olist_products_dataset.csv`  \n",
        "- `olist_sellers_dataset.csv`  \n",
        "- `olist_geolocation_dataset.csv`  \n",
        "- `product_category_name_translation.csv`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c94936eb",
      "metadata": {
        "id": "c94936eb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Ruta base configurable (MODIFICAR AQUÍ SEGÚN EL ENTORNO)\n",
        "DATA_DIR = Path('/content/drive/MyDrive/Maestria/M13/Olist')\n",
        "\n",
        "csv_files = [\n",
        "    'olist_orders_dataset.csv',\n",
        "    'olist_customers_dataset.csv',\n",
        "    'olist_order_items_dataset.csv',\n",
        "    'olist_order_payments_dataset.csv',\n",
        "    'olist_order_reviews_dataset.csv',\n",
        "    'olist_products_dataset.csv',\n",
        "    'olist_sellers_dataset.csv',\n",
        "    'olist_geolocation_dataset.csv',\n",
        "    'product_category_name_translation.csv'\n",
        "]\n",
        "\n",
        "print(\"Archivos esperados:\")\n",
        "for f in csv_files:\n",
        "    print(\" -\", DATA_DIR / f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "455ca342",
      "metadata": {
        "id": "455ca342"
      },
      "source": [
        "## 3. Carga de datos Olist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ab361fe",
      "metadata": {
        "id": "7ab361fe"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_olist_data(data_dir: Path):\n",
        "    \"\"\"Carga los 9 CSV de Olist y aplica parsers básicos de fechas.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data_dir : Path\n",
        "        Carpeta donde se encuentran los archivos CSV.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Diccionario con cada DataFrame.\n",
        "    \"\"\"\n",
        "    orders = pd.read_csv(\n",
        "        data_dir / 'olist_orders_dataset.csv',\n",
        "        parse_dates=[\n",
        "            'order_purchase_timestamp',\n",
        "            'order_approved_at',\n",
        "            'order_delivered_carrier_date',\n",
        "            'order_delivered_customer_date',\n",
        "            'order_estimated_delivery_date'\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    customers = pd.read_csv(data_dir / 'olist_customers_dataset.csv')\n",
        "\n",
        "    order_items = pd.read_csv(\n",
        "        data_dir / 'olist_order_items_dataset.csv',\n",
        "        parse_dates=['shipping_limit_date']\n",
        "    )\n",
        "\n",
        "    order_payments = pd.read_csv(data_dir / 'olist_order_payments_dataset.csv')\n",
        "\n",
        "    order_reviews = pd.read_csv(\n",
        "        data_dir / 'olist_order_reviews_dataset.csv',\n",
        "        parse_dates=['review_creation_date', 'review_answer_timestamp']\n",
        "    )\n",
        "\n",
        "    products = pd.read_csv(data_dir / 'olist_products_dataset.csv')\n",
        "    sellers = pd.read_csv(data_dir / 'olist_sellers_dataset.csv')\n",
        "    geolocation = pd.read_csv(data_dir / 'olist_geolocation_dataset.csv')\n",
        "    prod_cat_trans = pd.read_csv(data_dir / 'product_category_name_translation.csv')\n",
        "\n",
        "    data = {\n",
        "        'orders': orders,\n",
        "        'customers': customers,\n",
        "        'order_items': order_items,\n",
        "        'order_payments': order_payments,\n",
        "        'order_reviews': order_reviews,\n",
        "        'products': products,\n",
        "        'sellers': sellers,\n",
        "        'geolocation': geolocation,\n",
        "        'prod_cat_trans': prod_cat_trans\n",
        "    }\n",
        "    return data\n",
        "\n",
        "# Cargar datos\n",
        "data = load_olist_data(DATA_DIR)\n",
        "\n",
        "for name, df in data.items():\n",
        "    print(f\"{name:15s}: {df.shape[0]:7d} filas x {df.shape[1]:2d} columnas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61fe0fd8",
      "metadata": {
        "id": "61fe0fd8"
      },
      "source": [
        "\n",
        "## 4. EDA inicial resumido (Sprint 1)\n",
        "\n",
        "En el Sprint 1 se realizó un EDA más detallado en otro notebook.  \n",
        "Aquí se resume lo esencial necesario para continuar el pipeline:\n",
        "\n",
        "- **Rango temporal de las órdenes.**  \n",
        "- **Número de clientes y pedidos.**  \n",
        "- **Variables RFM básicas a nivel cliente.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19a0796c",
      "metadata": {
        "id": "19a0796c"
      },
      "outputs": [],
      "source": [
        "\n",
        "orders = data['orders']\n",
        "customers = data['customers']\n",
        "\n",
        "print(\"Rango temporal de `order_purchase_timestamp`:\")\n",
        "print(\"  Mínimo:\", orders['order_purchase_timestamp'].min())\n",
        "print(\"  Máximo:\", orders['order_purchase_timestamp'].max())\n",
        "print(\"  Duración (días):\",\n",
        "      (orders['order_purchase_timestamp'].max() - orders['order_purchase_timestamp'].min()).days)\n",
        "\n",
        "print(\"\\nNúmero de pedidos y clientes:\")\n",
        "print(\"  Pedidos totales:\", len(orders))\n",
        "print(\"  Clientes (customer_id) únicos:\", orders['customer_id'].nunique())\n",
        "print(\"  Clientes (customer_unique_id) únicos:\", customers['customer_unique_id'].nunique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fdcb498",
      "metadata": {
        "id": "9fdcb498"
      },
      "outputs": [],
      "source": [
        "\n",
        "def missing_values_table(df: pd.DataFrame, max_rows: int = 20):\n",
        "    \"\"\"Resumen de valores faltantes por columna.\"\"\"\n",
        "    mv = df.isna().sum()\n",
        "    mv = mv[mv > 0].sort_values(ascending=False)\n",
        "    res = pd.DataFrame({\n",
        "        'missing_count': mv,\n",
        "        'missing_pct': mv / len(df) * 100\n",
        "    })\n",
        "    return res.head(max_rows)\n",
        "\n",
        "print(\"Valores faltantes en orders:\")\n",
        "display(missing_values_table(orders))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d4b0cd2",
      "metadata": {
        "id": "0d4b0cd2"
      },
      "source": [
        "\n",
        "## 5. Diseño del pipeline reproducible (Sprint 2)\n",
        "\n",
        "A partir de aquí se define un **pipeline modular** que será la base del MVP:\n",
        "\n",
        "1. **Ingesta:** lectura de los CSV (ya implementado en `load_olist_data`).  \n",
        "2. **Data Cleaning:** tratamiento de valores faltantes, tipos y filtros básicos.  \n",
        "3. **Master Table a nivel pedido:** `orders_enriched`.  \n",
        "4. **Agregación y Feature Engineering a nivel cliente:** `customer_features`.  \n",
        "5. **Definición de target (preliminar y final).**  \n",
        "6. **Preparación de datos para modelado.**  \n",
        "7. **Entrenamiento y evaluación de modelos.**\n",
        "\n",
        "El objetivo es que este pipeline pueda ejecutarse **de extremo a extremo** (end‑to‑end)\n",
        "y, en el futuro, adaptarse a una ingesta mensual de nuevos datos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a8f4bb6",
      "metadata": {
        "id": "5a8f4bb6"
      },
      "source": [
        "### 5.1. Funciones de limpieza básica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd79885c",
      "metadata": {
        "id": "fd79885c"
      },
      "outputs": [],
      "source": [
        "\n",
        "def clean_orders(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Limpieza básica de la tabla de órdenes.\n",
        "\n",
        "    - Elimina filas sin `order_purchase_timestamp`.\n",
        "    - Filtra estados desconocidos si es necesario.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df = df[~df['order_purchase_timestamp'].isna()]\n",
        "    return df\n",
        "\n",
        "def clean_customers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Limpieza básica de clientes.\n",
        "\n",
        "    (Aquí se podría:\n",
        "     - homogeneizar ciudades/estados,\n",
        "     - tratar zip codes,\n",
        "     - eliminar duplicados, etc.)\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df = df.drop_duplicates(subset=['customer_id'])\n",
        "    return df\n",
        "\n",
        "def clean_order_payments(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    return df\n",
        "\n",
        "def clean_order_reviews(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    return df\n",
        "\n",
        "def clean_order_items(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    return df\n",
        "\n",
        "# Aplicar limpieza\n",
        "orders_clean = clean_orders(data['orders'])\n",
        "customers_clean = clean_customers(data['customers'])\n",
        "order_items_clean = clean_order_items(data['order_items'])\n",
        "order_payments_clean = clean_order_payments(data['order_payments'])\n",
        "order_reviews_clean = clean_order_reviews(data['order_reviews'])\n",
        "\n",
        "print(\"Limpieza básica aplicada.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "718d20e4",
      "metadata": {
        "id": "718d20e4"
      },
      "source": [
        "## 6. Master table a nivel pedido: `orders_enriched`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8573e5aa",
      "metadata": {
        "id": "8573e5aa"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_orders_enriched(\n",
        "    orders: pd.DataFrame,\n",
        "    customers: pd.DataFrame,\n",
        "    order_items: pd.DataFrame,\n",
        "    order_payments: pd.DataFrame,\n",
        "    order_reviews: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Construye una tabla maestra a nivel pedido.\n",
        "\n",
        "    Pasos:\n",
        "    1. Orders + Customers.\n",
        "    2. Agregación de pagos por `order_id`.\n",
        "    3. Agregación de reviews por `order_id`.\n",
        "    4. Agregación de items por `order_id` (precio, flete, nº ítems, nº sellers).\n",
        "    5. Cálculo de tiempos logísticos (demoras).\n",
        "    6. Filtro a órdenes entregadas (`delivered`).\n",
        "    \"\"\"\n",
        "    # 1) Orders + Customers\n",
        "    df = orders.merge(customers, on='customer_id', how='left')\n",
        "\n",
        "    # 2) Pagos agregados\n",
        "    payments_agg = order_payments.groupby('order_id').agg(\n",
        "        total_payment=('payment_value', 'sum'),\n",
        "        avg_installments=('payment_installments', 'mean')\n",
        "    ).reset_index()\n",
        "\n",
        "    df = df.merge(payments_agg, on='order_id', how='left')\n",
        "\n",
        "    # 3) Reviews agregadas\n",
        "    reviews_agg = order_reviews.groupby('order_id').agg(\n",
        "        avg_review_score=('review_score', 'mean'),\n",
        "        n_reviews=('review_score', 'size')\n",
        "    ).reset_index()\n",
        "\n",
        "    df = df.merge(reviews_agg, on='order_id', how='left')\n",
        "\n",
        "    # 4) Items agregados\n",
        "    items_agg = order_items.groupby('order_id').agg(\n",
        "        n_items=('order_item_id', 'count'),\n",
        "        n_sellers=('seller_id', 'nunique'),\n",
        "        total_items_price=('price', 'sum'),\n",
        "        total_freight_value=('freight_value', 'sum')\n",
        "    ).reset_index()\n",
        "\n",
        "    items_agg['order_value'] = items_agg['total_items_price'] + items_agg['total_freight_value']\n",
        "    df = df.merge(items_agg, on='order_id', how='left')\n",
        "\n",
        "    # 5) Tiempos logísticos\n",
        "    df['purchase_to_approval_days'] = (\n",
        "        (df['order_approved_at'] - df['order_purchase_timestamp']).dt.total_seconds() / 86400\n",
        "    )\n",
        "    df['approval_to_carrier_days'] = (\n",
        "        (df['order_delivered_carrier_date'] - df['order_approved_at']).dt.total_seconds() / 86400\n",
        "    )\n",
        "    df['carrier_to_customer_days'] = (\n",
        "        (df['order_delivered_customer_date'] - df['order_delivered_carrier_date']).dt.total_seconds() / 86400\n",
        "    )\n",
        "    df['delivery_delay_days'] = (\n",
        "        (df['order_delivered_customer_date'] - df['order_estimated_delivery_date']).dt.total_seconds() / 86400\n",
        "    )\n",
        "\n",
        "    # 6) Quedarnos solo con órdenes entregadas\n",
        "    df_completed = df[df['order_status'] == 'delivered'].copy()\n",
        "\n",
        "    return df_completed\n",
        "\n",
        "orders_enriched = build_orders_enriched(\n",
        "    orders_clean,\n",
        "    customers_clean,\n",
        "    order_items_clean,\n",
        "    order_payments_clean,\n",
        "    order_reviews_clean\n",
        ")\n",
        "\n",
        "print(\"orders_enriched:\", orders_enriched.shape)\n",
        "display(orders_enriched.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2ca8485",
      "metadata": {
        "id": "b2ca8485"
      },
      "source": [
        "## 7. Agregación por cliente y Feature Engineering ampliado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f66917d3",
      "metadata": {
        "id": "f66917d3"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_customer_features(orders_enriched: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Agrega información a nivel `customer_unique_id` y crea features de comportamiento.\n",
        "\n",
        "    Incluye:\n",
        "    - Variables RFM.\n",
        "    - Variables logísticas.\n",
        "    - Variables monetarias.\n",
        "    - Diversidad de compras (sellers, items).\n",
        "    - Indicadores de reviews.\n",
        "    \"\"\"\n",
        "    df = orders_enriched.copy()\n",
        "\n",
        "    # Aseguramos existencia de columnas clave\n",
        "    max_date = df['order_purchase_timestamp'].max()\n",
        "\n",
        "    agg_dict = {\n",
        "        'order_purchase_timestamp': ['min', 'max', 'count'],\n",
        "        'order_value': ['sum', 'mean', 'std', 'median', 'max'],\n",
        "        'total_items_price': ['sum', 'mean', 'std'],\n",
        "        'total_freight_value': ['sum', 'mean', 'std'],\n",
        "        'n_items': ['sum', 'mean', 'max'],\n",
        "        'n_sellers': ['sum', 'mean', 'max'],\n",
        "        'total_payment': ['sum', 'mean'],\n",
        "        'avg_installments': ['mean'],\n",
        "        'avg_review_score': ['mean', 'std'],\n",
        "        'n_reviews': ['sum'],\n",
        "        'purchase_to_approval_days': ['mean', 'std'],\n",
        "        'approval_to_carrier_days': ['mean', 'std'],\n",
        "        'carrier_to_customer_days': ['mean', 'std'],\n",
        "        'delivery_delay_days': ['mean', 'std', lambda x: (x > 0).sum(), lambda x: (x < 0).sum()]\n",
        "    }\n",
        "\n",
        "    customer_features = (\n",
        "        df.groupby('customer_unique_id')\n",
        "          .agg(agg_dict)\n",
        "          .reset_index()\n",
        "    )\n",
        "\n",
        "    # Renombrar columnas multi-índice\n",
        "    customer_features.columns = [\n",
        "        '_'.join(col).strip('_') if isinstance(col, tuple) else col\n",
        "        for col in customer_features.columns.values\n",
        "    ]\n",
        "\n",
        "    # Renombrados más amigables para algunas columnas clave\n",
        "    rename_map = {\n",
        "        'order_purchase_timestamp_min': 'first_purchase_date',\n",
        "        'order_purchase_timestamp_max': 'last_purchase_date',\n",
        "        'order_purchase_timestamp_count': 'frequency',\n",
        "        'order_value_sum': 'monetary_total',\n",
        "        'order_value_mean': 'monetary_avg',\n",
        "        'order_value_std': 'monetary_std',\n",
        "        'order_value_median': 'monetary_median',\n",
        "        'order_value_max': 'monetary_max',\n",
        "        'total_items_price_sum': 'items_price_total',\n",
        "        'total_freight_value_sum': 'freight_total',\n",
        "        'delivery_delay_days_mean': 'avg_delivery_delay',\n",
        "        'delivery_delay_days_<lambda_0>': 'num_delayed_orders',\n",
        "        'delivery_delay_days_<lambda_1>': 'num_early_orders',\n",
        "        'avg_review_score_mean': 'avg_review_score',\n",
        "        'avg_review_score_std': 'std_review_score',\n",
        "        'n_reviews_sum': 'n_reviews'\n",
        "    }\n",
        "    customer_features = customer_features.rename(columns=rename_map)\n",
        "\n",
        "    # Variables RFM & derivadas\n",
        "    customer_features['recency_days'] = (\n",
        "        (max_date - customer_features['last_purchase_date']).dt.days\n",
        "    )\n",
        "    customer_features['customer_lifetime_days'] = (\n",
        "        (customer_features['last_purchase_date'] - customer_features['first_purchase_date']).dt.days\n",
        "    )\n",
        "    customer_features['avg_days_between_orders'] = (\n",
        "        customer_features['customer_lifetime_days'] / (customer_features['frequency'] - 1)\n",
        "    )\n",
        "    customer_features['avg_days_between_orders'] = customer_features['avg_days_between_orders'].replace(\n",
        "        [np.inf, -np.inf], np.nan\n",
        "    ).fillna(0)\n",
        "\n",
        "    # Porcentaje de órdenes atrasadas y adelantadas\n",
        "    customer_features['pct_delayed_orders'] = (\n",
        "        customer_features['num_delayed_orders'] / customer_features['frequency']\n",
        "    ).fillna(0)\n",
        "    customer_features['pct_early_orders'] = (\n",
        "        customer_features['num_early_orders'] / customer_features['frequency']\n",
        "    ).fillna(0)\n",
        "\n",
        "    # Join con estado del cliente (primer estado observado)\n",
        "    state_map = (\n",
        "        df.groupby('customer_unique_id')['customer_state']\n",
        "          .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else x.iloc[0])\n",
        "    )\n",
        "    customer_features = customer_features.merge(\n",
        "        state_map.reset_index(),\n",
        "        on='customer_unique_id',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    return customer_features\n",
        "\n",
        "customer_features = build_customer_features(orders_enriched)\n",
        "print(\"customer_features:\", customer_features.shape)\n",
        "display(customer_features.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "674ad4d2",
      "metadata": {
        "id": "674ad4d2"
      },
      "source": [
        "## 8. Definición de target de churn (preliminar y final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d501f4e",
      "metadata": {
        "id": "8d501f4e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Target preliminar (Sprint 1): recency > 90 días\n",
        "PRELIM_CHURN_THRESHOLD = 90\n",
        "\n",
        "customer_features['churn_prelim'] = (\n",
        "    customer_features['recency_days'] > PRELIM_CHURN_THRESHOLD\n",
        ").astype(int)\n",
        "\n",
        "# Resumen de distribución preliminar\n",
        "def print_churn_distribution(label_col: str):\n",
        "    churn_count = customer_features[label_col].sum()\n",
        "    total = len(customer_features)\n",
        "    active_count = total - churn_count\n",
        "    churn_rate = churn_count / total * 100\n",
        "    print(f\"Target: {label_col}\")\n",
        "    print(f\"  Clientes totales : {total:6d}\")\n",
        "    print(f\"  Churned (1)      : {churn_count:6d} ({churn_rate:5.1f}%)\")\n",
        "    print(f\"  Activos  (0)     : {active_count:6d} ({100 - churn_rate:5.1f}%)\\n\")\n",
        "\n",
        "print_churn_distribution('churn_prelim')\n",
        "\n",
        "# Target final (Sprint 2) – EJEMPLO:\n",
        "# Se usa un umbral más laxo para reducir el desbalance, por ejemplo 365 días.\n",
        "FINAL_CHURN_THRESHOLD = 365\n",
        "\n",
        "customer_features['churn_final'] = (\n",
        "    customer_features['recency_days'] > FINAL_CHURN_THRESHOLD\n",
        ").astype(int)\n",
        "\n",
        "print_churn_distribution('churn_final')\n",
        "\n",
        "# Seleccionaremos 'churn_final' como target para modelado\n",
        "TARGET_COL = 'churn_final'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3e049ee",
      "metadata": {
        "id": "a3e049ee"
      },
      "source": [
        "### 8.1. Correlaciones básicas con el target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "763d5f1b",
      "metadata": {
        "id": "763d5f1b"
      },
      "outputs": [],
      "source": [
        "\n",
        "numeric_vars = [\n",
        "    col for col in customer_features.select_dtypes(include=[np.number]).columns\n",
        "    if col not in ['churn_prelim', 'churn_final']\n",
        "]\n",
        "\n",
        "corr_rows = []\n",
        "for var in numeric_vars:\n",
        "    corr = customer_features[[var, TARGET_COL]].corr().iloc[0, 1]\n",
        "    corr_rows.append({'Variable': var, 'Correlation': corr, 'Abs': abs(corr)})\n",
        "\n",
        "corr_df = (\n",
        "    pd.DataFrame(corr_rows)\n",
        "    .dropna()\n",
        "    .sort_values('Abs', ascending=False)\n",
        ")\n",
        "\n",
        "print(\"Top 15 correlaciones (por |correlación|):\\n\")\n",
        "for _, row in corr_df.head(15).iterrows():\n",
        "    strength = (\n",
        "        \"FUERTE\" if abs(row['Correlation']) > 0.5\n",
        "        else \"MODERADA\" if abs(row['Correlation']) > 0.3\n",
        "        else \"DÉBIL\"\n",
        "    )\n",
        "    print(f\"  {row['Variable']:35s}: {row['Correlation']:+.3f} ({strength})\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(\n",
        "    data=corr_df.head(15),\n",
        "    y='Variable', x='Correlation',\n",
        "    palette=['red' if c > 0 else 'blue' for c in corr_df.head(15)['Correlation']]\n",
        ")\n",
        "plt.axvline(0, color='black', linewidth=0.8)\n",
        "plt.title('Top correlaciones con el target')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c6713df",
      "metadata": {
        "id": "3c6713df"
      },
      "source": [
        "## 9. Preparación de datos para modelado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79401d1d",
      "metadata": {
        "id": "79401d1d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Seleccionamos variables numéricas y categóricas simples\n",
        "feature_cols_numeric = [\n",
        "    col for col in customer_features.select_dtypes(include=[np.number]).columns\n",
        "    if col not in ['churn_prelim', 'churn_final']\n",
        "]\n",
        "feature_cols_categ = ['customer_state']\n",
        "\n",
        "X_num = customer_features[feature_cols_numeric].fillna(0)\n",
        "X_cat = pd.get_dummies(customer_features[feature_cols_categ], drop_first=True)\n",
        "\n",
        "X = pd.concat([X_num, X_cat], axis=1)\n",
        "y = customer_features[TARGET_COL]\n",
        "\n",
        "print(\"Shape de X, y:\", X.shape, y.shape)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Train / Test listos para modelado.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "324bd54d",
      "metadata": {
        "id": "324bd54d"
      },
      "source": [
        "## 10. Modelos de clasificación"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08ceb7d0",
      "metadata": {
        "id": "08ceb7d0"
      },
      "source": [
        "### 10.1. Modelo base: Regresión Logística"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba4b053f",
      "metadata": {
        "id": "ba4b053f"
      },
      "outputs": [],
      "source": [
        "\n",
        "log_reg = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    class_weight='balanced'  # ayuda en caso de desbalance residual\n",
        ")\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_lr = log_reg.predict(X_test_scaled)\n",
        "y_proba_lr = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(\"Reporte de clasificación – Regresión Logística\")\n",
        "print(classification_report(y_test, y_pred_lr, digits=3))\n",
        "\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba_lr))\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(\n",
        "    y_test, y_pred_lr,\n",
        "    display_labels=['Activo', 'Churn'],\n",
        "    cmap='Blues'\n",
        ")\n",
        "plt.title('Matriz de confusión – Regresión Logística')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b712b894",
      "metadata": {
        "id": "b712b894"
      },
      "source": [
        "### 10.2. Modelo 2: Random Forest (y selección de variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "218bf330",
      "metadata": {
        "id": "218bf330"
      },
      "outputs": [],
      "source": [
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=None,\n",
        "    random_state=42,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "rf.fit(X_train, y_train)  # Random Forest maneja bien escalas distintas\n",
        "\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "y_proba_rf = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Reporte de clasificación – Random Forest\")\n",
        "print(classification_report(y_test, y_pred_rf, digits=3))\n",
        "\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba_rf))\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(\n",
        "    y_test, y_pred_rf,\n",
        "    display_labels=['Activo', 'Churn'],\n",
        "    cmap='Greens'\n",
        ")\n",
        "plt.title('Matriz de confusión – Random Forest')\n",
        "plt.show()\n",
        "\n",
        "# Importancias de variables (para redimensionar / seleccionar)\n",
        "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "importances = importances.sort_values(ascending=False)\n",
        "\n",
        "print(\"\\nTop 20 features más importantes según Random Forest:\")\n",
        "display(importances.head(20).to_frame('importance'))\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=importances.head(20), y=importances.head(20).index)\n",
        "plt.title('Importancia de las 20 principales variables')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f15ecf8b",
      "metadata": {
        "id": "f15ecf8b"
      },
      "source": [
        "## 11. Métricas de negocio y pruebas estadísticas (Chi², ANOVA/F‑test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20457650",
      "metadata": {
        "id": "20457650"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Ejemplo sencillo de pruebas estadísticas sobre un subconjunto de variables\n",
        "\n",
        "# Para Chi² se requieren variables no negativas\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler_mm = MinMaxScaler()\n",
        "X_chi2 = scaler_mm.fit_transform(X_num)\n",
        "\n",
        "chi2_vals, chi2_p = chi2(X_chi2, y)\n",
        "chi2_results = pd.DataFrame({\n",
        "    'feature': feature_cols_numeric,\n",
        "    'chi2_stat': chi2_vals,\n",
        "    'p_value': chi2_p\n",
        "}).sort_values('p_value')\n",
        "\n",
        "print(\"Top 10 variables (Chi², menor p-value):\")\n",
        "display(chi2_results.head(10))\n",
        "\n",
        "# ANOVA / F-test (para relación lineal con el target)\n",
        "f_vals, f_p = f_classif(X_num, y)\n",
        "f_results = pd.DataFrame({\n",
        "    'feature': feature_cols_numeric,\n",
        "    'f_stat': f_vals,\n",
        "    'p_value': f_p\n",
        "}).sort_values('p_value')\n",
        "\n",
        "print(\"\\nTop 10 variables (ANOVA F-test, menor p-value):\")\n",
        "display(f_results.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faae5267",
      "metadata": {
        "id": "faae5267"
      },
      "source": [
        "### 11.1. KPIs de churn y retención"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6098e6e",
      "metadata": {
        "id": "f6098e6e"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_basic_kpis(df_cf: pd.DataFrame, target_col: str):\n",
        "    churn_rate = df_cf[target_col].mean()\n",
        "    retention_rate = 1 - churn_rate\n",
        "    avg_monetary = df_cf['monetary_total'].mean()\n",
        "    active_monetary = df_cf.loc[df_cf[target_col] == 0, 'monetary_total'].mean()\n",
        "    churned_monetary = df_cf.loc[df_cf[target_col] == 1, 'monetary_total'].mean()\n",
        "\n",
        "    print(f\"Churn rate        : {churn_rate*100:5.2f}%\")\n",
        "    print(f\"Retention rate    : {retention_rate*100:5.2f}%\")\n",
        "    print(f\"Gasto promedio total por cliente : {avg_monetary:,.2f}\")\n",
        "    print(f\"Gasto promedio clientes activos  : {active_monetary:,.2f}\")\n",
        "    print(f\"Gasto promedio clientes churn    : {churned_monetary:,.2f}\")\n",
        "\n",
        "compute_basic_kpis(customer_features, TARGET_COL)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffe4cb6f",
      "metadata": {
        "id": "ffe4cb6f"
      },
      "source": [
        "\n",
        "## 12. Notas sobre versionamiento (Git) y pipeline mensual\n",
        "\n",
        "- Este notebook debe estar versionado en un repositorio Git, junto con:\n",
        "  - El notebook original del **Sprint 1**.\n",
        "  - Un README explicando:\n",
        "    - Objetivo del proyecto.\n",
        "    - Estructura de carpetas.\n",
        "    - Cómo ejecutar el pipeline de principio a fin.\n",
        "- Sugerencia de pasos mínimos en Git:\n",
        "  1. `git init` en la carpeta del proyecto.  \n",
        "  2. Añadir `.gitignore` (por ejemplo, para datos crudos si no se suben).  \n",
        "  3. `git add .`  \n",
        "  4. `git commit -m \"Sprint 1 y 2 - pipeline churn Olist\"`  \n",
        "\n",
        "### Simulación de ejecución mensual\n",
        "\n",
        "La idea para futuras iteraciones es:\n",
        "\n",
        "1. Cada mes se agregan nuevas filas a `olist_orders_dataset.csv` y tablas relacionadas.  \n",
        "2. El pipeline de este notebook se encapsula en funciones (por ejemplo `run_monthly_pipeline(fecha_corte)`),\n",
        "   que vuelven a:\n",
        "   - cargar datos,  \n",
        "   - construir `orders_enriched`,  \n",
        "   - recalcular `customer_features`,  \n",
        "   - marcar `churn_final` según la ventana de recencia definida,  \n",
        "   - generar un ranking actualizado de clientes en riesgo.\n",
        "\n",
        "De esta forma, el MVP queda listo para integrarse posteriormente a un dashboard o a un sistema operativo de campañas de retención.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}